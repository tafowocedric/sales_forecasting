{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Illustration Guide to RNN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Definition</h3>\n",
    "<p>RNN are neural network at are good at modeling sequence data.</p>\n",
    "\n",
    "<h2>Example</h2>\n",
    "<p>\n",
    "    Let picture a ball in motion.\n",
    "without enough data you would know the direction of where the ball is going.\n",
    "</p>\n",
    "\n",
    "<h5>Type of RNN</h5>\n",
    "<ul>\n",
    "    <li>Vanilla RNN</li>\n",
    "    <li>LSTM</li>\n",
    "    <li>GRU</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Use cases</h1>\n",
    "<ul>\n",
    "    <li>Text</li>\n",
    "    <li>Images</li>\n",
    "    <li>Audio</li>\n",
    "</ul>\n",
    "\n",
    "<p>They use sequential memory.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example</h2>\n",
    "<ul>\n",
    "    <ol>try saying the alphabet</ol> \n",
    "    <ol>Now try doing it backwards.</ol>\n",
    "    <ol>Now start with the letter F</ol>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "It difficult because your brain processes data as a sequence.\n",
    "<br>\n",
    "RNN has the ability to recognize sequence patterns but how does it do?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let look at our traditional feedforward NN\n",
    "<img src=\"https://miro.medium.com/max/480/0*3ksovSKqmW2xj2v3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This representation is a hidden state. which is a representation of previously used state.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1600/0*ppdFWgAZQXgawgGl.png\" />\n",
    "\n",
    "Now let try to see it in action.\n",
    "\n",
    "<h2>Example</h2>\n",
    "<p>What time is it?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem</h2>\n",
    "<p>How RNN face a problem of short term memory state. Caused by the infinit vanishing gradient problem, which is common in other RNN networking structures.\n",
    "    </p>\n",
    "\n",
    "As the RNN processes more steps it has problems recalling the previous states.\n",
    "\n",
    "We do a forward probagation\n",
    "calculate the E or how bad the model is from our actual value\n",
    "last it uses that E value to do back probagation with calculates the gradients for each node in the network.\n",
    "\n",
    "The gradient is a value use to adjust the network internal weights allowing the network to learn. the bigger the gradient the bigger the adjustment, and vice verse. \n",
    "\n",
    "Here is where the problem lays when doing backprobagation, each nodes calculates it gradient in effect to the gradient of the layer before it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1_yBXV9o5q7L_CvY7quJt3WQ.png\" />\n",
    "\n",
    "Long short term memory\n",
    "Gated recurrent unit.\n",
    "GRU are capable of learning long term dependencies by a magnetism called gate. These gates are different tensor operations that can learn what information to add or remove in the hidden state. Because of this LTM is not an issue for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://liyanxu.blog/wp-content/uploads/2019/01/Screen-Shot-2019-01-24-at-19.46.54.png\" style=\"height: 400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=8HyCNIVRbSU&ab_channel=TheA.I.Hacker-MichaelPhi\">Illustrated Guide to LSTM's and GRU's: A step by step explanation</a> <br><br>\n",
    "<a href=\"http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/\">Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and TheanoRecurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano</a> <br><br>\n",
    "<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a> <br><br>\n",
    "<a href=\"https://www.youtube.com/watch?v=WCUNPb-5EYI&ab_channel=BrandonRohrer\">Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
